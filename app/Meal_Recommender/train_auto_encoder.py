# -*- coding: utf-8 -*-
"""train_auto_encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dg3JIQACzW1HC9XDX-skxjpG_4_ksGLw

file name for cluster csv: {cluster_num}_ids.csv <br>
first schedule to run train_auto_encoder() func; param: dict of cluster num: csv path;  returns preds for all clusters <br>
then during prediction run predict_auto_encoder(); param: cluster_num, user_id <br>
returns pred list
"""

import tensorflow as tf
import math
import pandas as pd
import numpy as np

def Load_Data(csvpath, ColumnList, sep=None):
  
  print("Loading data from ", csvpath)
  if sep==None:
    data = pd.read_csv(csvpath)[ColumnList]
  else:
    data = pd.read_csv(csvpath, sep=sep)[ColumnList]
  print ("number of rows, cols ", data.shape)
  if 'customers' in csvpath:
    print ("unique cust: ",len(data['USER_ID'].unique()))
  elif 'products' in csvpath:
    print ("unique prods: ", len(data['ProductID'].unique()))
  return data

def Create_Datasets(path, userid_col):    
    sales_data = Load_Data(path+"sales.csv", ["SalesID", userid_col, "ProductID", "Quantity", "SalesDate"], sep=';')
    customer_data_our = Load_Data(path+"USER.csv", [userid_col, "NAME", "EMAIL_ID","GENDER","AGE"])
    product_data_our = Load_Data(path+"PRODUCT.csv", ["ProductID", "ProductName"])
    
    return sales_data, customer_data_our, product_data_our

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')
# path = '/content/drive/'`

def Combine_Data(customer_data, sales_data, products_data, x, userid_col, path):
  

    print("Combining data...")
    customer_data = customer_data[customer_data[userid_col].isin(x)]
    sales_data = sales_data[sales_data[userid_col].isin(x)]
    sale_location_data = pd.merge(sales_data, customer_data, on=userid_col)
    
    Data = pd.merge(sale_location_data, products_data, on='ProductID')
  
    return Data

def Data_Processing(customer_data, sales_data, products_data, x, userid_col, path): 
  
  Data = Combine_Data(customer_data, sales_data, products_data, x, userid_col, path)
  
  print("Processing Data...")
  SortedData = Data.sort_values([userid_col, "SalesDate", "SalesID", "ProductID"], ascending=[True, True, True, True])
  Final_Data = Data.dropna(subset=[userid_col, "SalesID", "ProductID","SalesDate", "Quantity"])
  
  return Final_Data

def Get_Quantity(Data, Cust_id, Prod_id):
  
  Cust_Id_Filter = Data.loc[Data['USER_ID']==Cust_id]
  Prod_Id_Filter = Cust_Id_Filter.loc[Cust_Id_Filter['ProductID']==Prod_id]
  Prod_Quantity = np.sum(Prod_Id_Filter['Quantity'])
  return Prod_Quantity

def Build_CoOccurence_Mat(Data):
  
  Customer_Ids = Data['USER_ID'].unique()
  Product_Ids = Data['ProductID'].unique()
  
  No_Customers = Customer_Ids.shape[0]
  No_Features = Product_Ids.shape[0]
  print ("Building co occurrence matrix for ", No_Customers, " customers")
  mat = np.zeros((No_Customers, No_Features))
  
  for i in range(0,No_Customers) : 
#     print(" ->CustID ", Customer_Ids[i])
    for j in range(0,No_Features) : 
      
      
      Cust_Id = Customer_Ids[i]
      Prod_Id = Product_Ids[j]   
      IsPurchased = Get_Quantity(Data, Cust_Id, Prod_Id)
      
      if(IsPurchased > 0):
        mat[i][j] = 1
  
  return mat

def initialize_tensorflow(Final_Data, userid_col):
    Product_Ids = Final_Data['ProductID'].unique()
    Customer_Ids = Final_Data[userid_col].unique()

    No_Customers = Customer_Ids.shape[0]
    No_Products = Product_Ids.shape[0]

    num_input = No_Products
    num_hidden_1 = 10
    num_hidden_2 = 5

    X = tf.placeholder(tf.float64, [None, num_input])

    weights = {
        'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),
        'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),
        'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),
        'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),
    }

    biases = {
        'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),
        'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),
        'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),
        'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),
    }
    
    return weights, biases, X, Customer_Ids

def encoder(x, weights, biases):
    # Encoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))
    # Encoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))
    return layer_2


# Building the decoder

def decoder(x, weights, biases):
    # Decoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))
    # Decoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))
    return layer_2

# Construct model
def construct_model(X, weights, biases):
    encoder_op = encoder(X, weights, biases)
    decoder_op = decoder(encoder_op, weights, biases)
    # Prediction
    y_pred = decoder_op
    # Targets are the input data.
    y_true = X
    return y_true, y_pred, decoder_op

# Define loss and optimizer, minimize the squared error
def loss_optimize(y_true, y_pred):
    
    loss = tf.losses.mean_squared_error(y_true, y_pred)
    optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)

    

    # Define evaluation metrics

    eval_x = tf.placeholder(tf.int32, )
    eval_y = tf.placeholder(tf.int32, )
    pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)
    return optimizer, loss

def run_session(Mat, optimizer, loss, X, decoder_op):
    predictions = pd.DataFrame()
    init = tf.global_variables_initializer()
    local_init = tf.local_variables_initializer()
    
    with tf.Session() as session:

        epochs = 100
        batch_size = 50

        session.run(init)
        session.run(local_init)

        num_batches = math.ceil(Mat.shape[0] / batch_size)
        BatchIndex = 0


        for i in range(epochs):

            avg_cost = 0

            for batchNum in range(num_batches) :

                if(batchNum == num_batches-1):
                  batch = Mat.iloc[batchNum :, :]
                else :
                  batch = Mat.iloc[batchNum : batchNum+batch_size, :]

                _, l = session.run([optimizer, loss], feed_dict={X: batch})
                avg_cost += l

                batchNum += batch_size

            avg_cost /= num_batches

#             print("Epoch: {} Loss: {}".format(i + 1, avg_cost))

#         print("Predictions...")

        preds = session.run(decoder_op, feed_dict={X: Mat})

        predictions = predictions.append(pd.DataFrame(preds))
    return predictions

def GetNextPredProduct(N, Pred_Matrix, Customer_Ids):
  
  # returns top N most likely to buy products for each user
  # Data structure : Dict with userID key and list of N productIDs as value
  
  Pred_Products = {}
  
  No_Users = Pred_Matrix.shape[0]
  No_Products = Pred_Matrix.shape[1]
  
  for user in range(No_Users) :
    
    Pred_Row = np.array(Pred_Matrix.iloc[user, :])
    Sorted_Row = np.argsort(Pred_Row)[::-1][:N]
    Sorted_Row += 1
    
    userID = Customer_Ids[user]
    Pred_Products[userID] = Sorted_Row
    
  return Pred_Products

def write_dict_to_csv(dict, path):
  import csv

  w = csv.writer(open(path, "w"))
  for key, val in dict.items():
      w.writerow([key, val])

def run_model(Final_Data, Mat, userid_col, cluster_num):
    print ('Initializing weights, biases, X..')
    weights, biases, X, Customer_Ids = initialize_tensorflow(Final_Data, userid_col)
    
    print ('creating encoder, decoder..')
    y_true, y_pred, decoder_op = construct_model(X, weights, biases)

    print ('building optimizer, loss..')
    optimizer, loss = loss_optimize(y_true, y_pred)
    print ('ready to run..')
    predictions = run_session(Mat, optimizer, loss, X, decoder_op)
    print ('got the predictions!')
    Pred_Products = GetNextPredProduct(10, predictions, Customer_Ids)
    print ("writing to file..")
    import pprint
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(Pred_Products)
    write_dict_to_csv(Pred_Products, str(cluster_num)+"_preds.csv")
    print ('success!')

# run this function to train all clusters and get predictions
def train_auto_encoder(cluster_dict):
    user_id_col = "USER_ID"
    sales_data, customer_data, products_data = Create_Datasets('', user_id_col)
    # read csv for each cluster
    # {0: path, 1: path..}
    
    
    # for each cluster path, train the model
    for cluster_num, csv_path in cluster_dict.items():
        
        customer_id_df = pd.read_csv(csv_path,header=None)
        
        # gives list of customer ids to train on
        all_cust = customer_id_df.loc[0].tolist()
        print ('Processing begins for cluster num ',+cluster_num)
        Final_Data = Data_Processing(customer_data, sales_data, products_data, all_cust, user_id_col, '.\\')
        print ('Building CoOccurrence Mat')
        Mat = Build_CoOccurence_Mat(Final_Data)
        Mat = pd.DataFrame(Mat)
        print ('Matrix shape ',Mat.shape)
        run_model(Final_Data, Mat, user_id_col, cluster_num)

train_auto_encoder({0: '0_ids.csv', 1:'1_ids.csv', 2:'2_ids.csv', 3:'3_ids.csv', 4:'4_ids.csv', 5:'5_ids.csv'})